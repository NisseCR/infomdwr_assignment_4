
```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(text2vec)
library(wordcloud)
library(magrittr)
library(tidyverse)
library(tm)
library(textstem)
library(cluster)
library(factoextra)
library(mclust)

```

```{r}
#| label: data loading

set.seed(123)
data("movie_review")
```

# Data description

```{r}
#| label: eda histogram
#| warning: false

# Number of reviews
rows <- nrow(movie_review)

# TODO: Style histogram
movie_review %>% 
  mutate(length = str_count(review)) %>% 
  ggplot(aes(x = length)) +
  geom_histogram(bins = 50) +
  labs(x = "Number of words",
       y = "Frequency")
```

The data consists of `r rows` reviews of various movies. The histogram shows the distribution of their lengths. Most reviews seem to be on the shorter side, with less than 2500 words, but the distribution is majorly skewed. Upon further inspection, there seem to be a few data points that have such a big length, they don't show up on the histogram. One of these datapoints is the longest review, that has a length of 13708 words. Overall, reviews also don't appear to be shorter than roughly 1000 words. 

To start exploring the content of the reviews, a word cloud can be used to visualize the most common words in the corpus.

```{r}
#| label: eda wordcloud
#| warning: false

movie_review %$% wordcloud(review, 
                           min.freq = 10, 
                           max.words = 50, 
                           random.order = FALSE,
                           colors = brewer.pal(8, "Dark2"))


```

In the word cloud stop words like "the" and "also" are over represented, suggesting their removal would be beneficial as it would reduce dimensionality without impacting the clustering. That is because stop words are so frequent and carry relatively little meaning with them. Additionally, the words "movie" and "film" should also be removed for similar reasons. As all of the reviews concern this topic it is very likely they all mention either "movie" or "film" and thus, the words occurrence likely do not reflect any grouping within the reviews. Additionally, various forms of words "character" and "characters" 


# Text pre-processing

Describe text pre-processing steps you have used (approx. one or two paragraphs)

```{r}
#| label: pre-processing

preprocess <- function(review){
  # Cast all strings to lowercase
  review <- tolower(review)
  
  #Stem and lemmatize
  review <- lemmatize_strings(review)
  review <- stemDocument(review)
  
   # Remove punctuation, numbers, and extra whitespace
  review <- gsub("[[:punct:]]", "", review) 
  review <- gsub("[0-9]", "", review)       
  review <- gsub("\\s+", " ", review)        
  review <- str_squish(review)                    
  
  # Remove stopwords and custom words
  custom_stopwords <- c(stopwords("english"), "movie", "film")
  review <- removeWords(review, custom_stopwords)
  
  return(review)
}

movie_review$clean <- preprocess(movie_review$review)
```


# Text representaion

Briefly describe your text representation method. (approx. one or two paragraphs)

```{r}
# Tokinize the corpus

tokens <- word_tokenizer(movie_review$clean)
iterator <- itoken(tokens, progressbar = FALSE)

# Create vocabulary 
vocabulary <- create_vocabulary(iterator) %>% 
  prune_vocabulary(term_count_min = 5)

# Create a term co-occurence matrix
vectorizer <- vocab_vectorizer(vocabulary)
termoc_matrix <- create_tcm(iterator, vectorizer, skip_grams_window = 5)
```

```{r}
# Create the embeddings based on a GlobalVectors model
glove_model <- GlobalVectors$new(rank = 50, x_max = 10)
glove_embedding <- glove_model$fit_transform(termoc_matrix, n_iter = 20)
``` 

```{r}
embed_review <- function(review, embeddings){
  review_words <- unlist(strsplit(review, "\\W+"))
  relevant_embeds <- embeddings[review_words, , drop = FALSE]
  if(nrow(relevant_embeds) == 0){
    return(rep(NA, ncol(embeddings)))
  }
  return(colMeans(relevant_embeds, na.rm = TRUE))
}

embedding_matrix <- as.data.frame(glove_embedding)
row.names(embedding_matrix) <- rownames(glove_embedding)

movie_review <- movie_review %>%
  rowwise() %>%
  mutate(average_embedding = list(embed_review(review, embedding_matrix))) %>%
  unnest_wider(average_embedding, names_sep = "_")
```


# Text clustering

Briefly describe which models you compare to perform clustering. (approx. two or three paragraphs)

```{r}
review_embeddings <- movie_review[, 5: ncol(movie_review)]

get_kmean_cluster <- function(data, k){
  # K-means clustering
  kmeans_result <- kmeans(data, centers = k, nstart = 25, iter.max = 30)
  return(kmeans_result)
}

kmean_5 <- get_kmean_cluster(review_embeddings, 5)
movie_review$kmean_5 <- kmean_5$cluster

kmean_10 <- get_kmean_cluster(review_embeddings, 10)
movie_review$kmean_10 <- kmean_10$cluster
```

```{r}
gmm_5 <- Mclust(review_embeddings, G = 5)
movie_review$gmm_5 <- as.factor(gmm_5$classification)

gmm_10 <- Mclust(review_embeddings, G = 10)
movie_review$gmm_10 <- as.factor(gmm_10$classification)
```


# Evaluation & model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)


```{r}
#| label: table example
data.frame(
  model       = c("clustering model 1", "clustering model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

Topic modeling may perform badly (ex overfitting) when used on short texts. Which is the case here. https://lazarinastoy.com/topic-modelling-limitations-short-text/ 

# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
