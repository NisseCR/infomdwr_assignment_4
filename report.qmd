
```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(text2vec)
library(wordcloud)
library(magrittr)
library(tidyverse)
library(tm)
library(textstem)
library(cluster)
library(factoextra)
library(mclust)

```

```{r}
#| label: data loading

set.seed(123)
data("movie_review")
```

# Data description

```{r}
#| label: eda histogram
#| warning: false

# Number of reviews
rows <- nrow(movie_review)

movie_review %>% 
  mutate(length = str_count(review)) %>% 
  ggplot(aes(x = length)) +
  geom_histogram(fill = "steelblue", color = "black", alpha = 0.8, bins = 50) +
  labs(x = "Number of words",
       y = "Frequency") + 
  theme_minimal() +
  theme(
    panel.grid.major = element_line(size = 0.5, linetype = 'dashed', color = 'gray'),
    panel.grid.minor = element_blank()
  )
```

The data consists of `r rows` reviews of various movies. The histogram shows the distribution of their lengths. Most reviews seem to be on the shorter side, with less than 2500 words, but the distribution is majorly skewed. Upon further inspection, there seem to be a few data points that have such a big length, they don't show up on the histogram. One of these datapoints is the longest review, that has a length of 13708 words. Overall, reviews also don't appear to be shorter than roughly 1000 words. 

To start exploring the content of the reviews, a word cloud can be used to visualize the most common words in the corpus.

```{r}
#| label: eda wordcloud
#| warning: false

movie_review %$% wordcloud(review, 
                           min.freq = 10, 
                           max.words = 50, 
                           random.order = FALSE,
                           colors = brewer.pal(8, "Dark2"))


```

In the word cloud stop words like "the" and "also" are over represented, suggesting their removal would be beneficial as it would reduce dimensionality without impacting the clustering. That is because stop words are so frequent and carry relatively little meaning with them. Additionally, the words "movie" and "film" should also be removed for similar reasons. As all of the reviews concern this topic it is very likely they all mention either "movie" or "film" and thus, the words occurrence likely do not reflect any grouping within the reviews. Additionally, various forms of words "character" and "characters" 


# Text pre-processing

Describe text pre-processing steps you have used (approx. one or two paragraphs)

```{r}
#| label: pre-processing
#| warning: false

preprocess <- function(review){
  # Cast all strings to lowercase
  review <- tolower(review)
  
  #Stem and lemmatize
  review <- lemmatize_strings(review)
  review <- stemDocument(review)
  
   # Remove punctuation, numbers, and extra whitespace
  review <- gsub("[[:punct:]]", "", review) 
  review <- gsub("[0-9]", "", review)       
  review <- gsub("\\s+", " ", review)        
  review <- str_squish(review)                    
  
  # Remove stopwords and custom words
  custom_stopwords <- c(stopwords("english"), "movie", "film")
  review <- removeWords(review, custom_stopwords)
  
  return(review)
}

movie_review$clean <- preprocess(movie_review$review)
```


# Text representaion

Briefly describe your text representation method. (approx. one or two paragraphs)

```{r}
#| label: tokinzation
#| warning: false

# Tokinize the corpus
tokens <- word_tokenizer(movie_review$clean)
iterator <- itoken(tokens, progressbar = FALSE)

# Create vocabulary 
vocabulary <- create_vocabulary(iterator) %>% 
  prune_vocabulary(term_count_min = 5)

# Create a term co-occurence matrix
vectorizer <- vocab_vectorizer(vocabulary)
termoc_matrix <- create_tcm(iterator, vectorizer, skip_grams_window = 5)
```

```{r}
#| label: embedding_model
#| warning: false

# Create the embeddings, with 50 dimensions 
glove_model <- GlobalVectors$new(rank = 50, x_max = 10)
glove_embedding <- glove_model$fit_transform(termoc_matrix, n_iter = 20)
``` 

```{r}
#| label: average_embeddings
#| warning: false

# Custom function to get the average of each word embedding in a review
embed_review <- function(review, embeddings){
  review_words <- unlist(strsplit(review, "\\W+"))
  relevant_embeds <- embeddings[review_words, , drop = FALSE]
  if(nrow(relevant_embeds) == 0){
    return(rep(NA, ncol(embeddings)))
  }
  return(colMeans(relevant_embeds, na.rm = TRUE))
}

# Create dataframe for existing word embeddings instead of matrix
embedding_matrix <- as.data.frame(glove_embedding)
row.names(embedding_matrix) <- rownames(glove_embedding)

# Apply the custom function on every embedding dimension in a row
movie_review <- movie_review %>%
  rowwise() %>%
  mutate(average_embedding = list(embed_review(review, embedding_matrix))) %>%
  unnest_wider(average_embedding, names_sep = "_")
```


# Text clustering

Briefly describe which models you compare to perform clustering. (approx. two or three paragraphs)

```{r}
#| label: kmean_clustering
#| warning: false

# Separate features from the dataframe
review_embeddings <- movie_review[, 5: ncol(movie_review)]

# Custom function to apply k_mean clustering qith custom k
get_kmean_cluster <- function(data, k){
  kmeans_result <- kmeans(data, centers = k, nstart = 25, iter.max = 30)
  return(kmeans_result)
}

# Create clusters for k = 5
kmean_5 <- get_kmean_cluster(review_embeddings, 5)
movie_review$kmean_5 <- kmean_5$cluster

# Create clusters for k = 10
kmean_10 <- get_kmean_cluster(review_embeddings, 10)
movie_review$kmean_10 <- kmean_10$cluster
```

```{r}
#| label: gmm_clustering
#| warning: false

# Create gmm clusters with k = 5
gmm_5 <- Mclust(review_embeddings, G = 5)
movie_review$gmm_5 <- as.factor(gmm_5$classification)

# Create gmm clusters with k = 10
gmm_10 <- Mclust(review_embeddings, G = 10)
movie_review$gmm_10 <- as.factor(gmm_10$classification)
```


# Evaluation & model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

TODO: 
- Visualization: gg grid the plots, so they print nicer
- Visualization: change the colour pallete?
- Visualization: describe what we can see (ex gmm10 clearly shows that only 4 big clusters were detected, which is possible since it allows for varying cluster sizes)

- bootstrap stability for GMM - how to do it
- add Silhouette indecies for each

- present all indicies in a table
- describe the table 


Visual inspection of clusters: 

```{r}
#| label: Visual inspection with Umap
#| warning: false

library(ggplot2)
library(ggrepel)
library(plotly)
library(umap)

vizualization <- umap(review_embeddings, n_neighbors = 15, n_threads = 2)

df  <- data.frame(word = rownames(review_embeddings), 
                  xpos = gsub(".+//", "", rownames(review_embeddings)), 
                  x = vizualization$layout[, 1], y = vizualization$layout[, 2], 
                  stringsAsFactors = FALSE)

plot_ly(df, x = ~x, y = ~y, type = "scatter", color = ~movie_review$kmean_5) %>%
  layout(title = "Umap visuzalization of movie reviews with 5 kmeans clusters")
plot_ly(df, x = ~x, y = ~y, type = "scatter", color = ~movie_review$kmean_10) %>%
  layout(title = "Umap visuzalization of movie reviews with 10 kmeans clusters")
plot_ly(df, x = ~x, y = ~y, type = "scatter", color = ~movie_review$gmm_10) %>%
  layout(title = "Umap visuzalization of movie reviews with 5 gmm clusters")
plot_ly(df, x = ~x, y = ~y, type = "scatter", color = ~movie_review$gmm_5) %>%
  layout(title = "Umap visuzalization of movie reviews with 10 kmeans clusters")

movie_review <- movie_review %>%
  mutate(
    sentiment = as.factor(sentiment)
  ) 
plot_ly(df, x = ~x, y = ~y, type = "scatter", color = ~movie_review$sentiment) %>%
  layout(title = "Sentiment of Reviews")
```

Davies-Bouldin's index - measure of cluster separation

"The Davies-Bouldin Index is a validation metric that is used to evaluate clustering models. It is calculated as the average similarity measure of each cluster with the cluster most similar to it. In this context, similarity is defined as the ratio between inter-cluster and intra-cluster distances. As such, this index ranks well-separated clusters with less dispersion as having a better score." link: https://www.geeksforgeeks.org/davies-bouldin-index/

GMM 5 performes by far the best. 

```{r}
#| label: Davies-Bouldin's index
#| #| warning: false

library(clusterSim)
DB.index <- function(clusters){
  ind <- index.DB(review_embeddings, as.numeric(clusters), d=NULL, centrotypes="centroids", p=2, q=2)
  return(ind$DB)
}

DB.index(clusters = movie_review$kmean_5)
DB.index(clusters = movie_review$kmean_10)
DB.index(clusters = movie_review$gmm_5)
DB.index(clusters = movie_review$gmm_10)

```



 
How stable are the clusters: 

```{r}
#| label: bootstrap stability 
library(fpc)
boot_k5 <- clusterboot(review_embeddings, B = 100, clustermethod = kmeansCBI, k = 5, 
                      count = FALSE)
boot_k10 <- clusterboot(review_embeddings, B = 100, clustermethod = kmeansCBI, k = 10, 
                      count = FALSE)

# TODO: figure out how to run this for Mclust, the method is correct accrding to what I found 
#boot_g5 <- clusterboot(review_embeddings, B = 100, clustermethod = mclustBIC, k = 5, 
#                      count = FALSE, multipleboot=FALSE)

#boot_g10<- clusterboot(review_embeddings, B = 100, clustermethod = mclustBIC, k = 5, 
#                      count = FALSE, multipleboot=FALSE)

boot_k5$bootmean
```





```{r}
#| label: table example
data.frame(
  model       = c("clustering model 1", "clustering model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

Topic modeling may perform badly (ex overfitting) when used on short texts. Which is the case here. https://lazarinastoy.com/topic-modelling-limitations-short-text/ 


# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
